# -*- coding: utf-8 -*-
"""notebook02cf0054f2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/balaji072001/notebook02cf0054f2.1bab9841-00c1-426c-ab72-cf2523e883ae.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251008/auto/storage/goog4_request%26X-Goog-Date%3D20251008T093512Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6de8647e52ce16d47b3b654e1c24bd01cb8030f6732ce41fd39d475c74a8e5ac71473baaeb41fe65ff3f52d5a9fa9d8d8d5040d2a090346bfdebb7ea647f76c11a0063327762dd9122484f418087fb7930063ef3ed6d30e482c11f27b51bfdfd796e19003f458dd07ce7c82fa017db5896af8e1e9ed77fa27c5f8505a258b0b9ad90a8bf3c64cd9697aa268bc097c052a348c750c54333588ad2b39ee60b6ad27beebf37fbba272a269f164e779d870e61911c0523b17daf4b15bb95fc8340dd742f56e665723359172d04c8c703199cee471fd1edc504a0752c8f782508d4bfa3fd0f6c8a9bd8648ddaade4735e4a06cd345f1f4aee74bbf834542d5db97e1f
"""

pip install llama-cpp-python

!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf -O llama-2-7b-chat.gguf

import llama_cpp

model_path = "llama-2-7b-chat.gguf"  # Path to your downloaded model
llm = llama_cpp.Llama(model_path=model_path, n_ctx=2048)

def chatbot():
    print("Chatbot is ready (type 'exit' to quit)!")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit"]:
            print("Chatbot: Goodbye!")
            break
        prompt = f"User: {user_input}\nAI:"
        response = llm(prompt, max_tokens=250)
        print("Chatbot:", response['choices'][0]['text'].strip())

chatbot()

# Commented out IPython magic to ensure Python compatibility.
# %pip install Flask-ngrok pyngrok

!ngrok config add-authtoken 33mScix9m2Hqb350uGGTa1iB1l0_7wQUUdbZTbHZzpvH48iSe

from flask import Flask, request, jsonify
from pyngrok import ngrok
import llama_cpp

app = Flask(__name__)
llm = llama_cpp.Llama(model_path="llama-2-7b-chat.gguf", n_ctx=2048)

@app.route('/chat', methods=['POST'])
def chat():
    user_message = request.get_json().get('message', '')
    prompt = f"User: {user_message} AI:"
    response = llm(prompt, max_tokens=250)
    return jsonify({'response': response['choices'][0]['text'].strip()})

public_url = ngrok.connect(5000)
print("ngrok tunnel:", public_url)

app.run(port=5000)

